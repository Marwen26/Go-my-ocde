{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eac659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.1: function to Get and parse html content\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "LINK = requests.get(\"https://en.wikipedia.org/wiki/Python_(programming_language)\")\n",
    "cont = LINK.content  #Web page content\n",
    "soup = BeautifulSoup(cont,'lxml')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0339e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.2: function to Extract article title\n",
    "\n",
    "import  requests\n",
    "from bs4 import BeautifulSoup\n",
    "L = requests.get(\"https://en.wikipedia.org/wiki/Python_(programming_language)\")\n",
    "C = L.content\n",
    "S = BeautifulSoup(C,\"lxml\")\n",
    "T = S.find('title')\n",
    "print(T.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca5cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''''Q1.3: Extract article text for each paragraph with their respective headings. \n",
    "          Map those headings to their respective paragraphs in the dictionary''''\n",
    "\n",
    "from bs4 import  BeautifulSoup\n",
    "import requests\n",
    "LINK = requests.get(\"https://en.wikipedia.org/wiki/Python_(programming_language)\")\n",
    "Con = LINK.content\n",
    "S = BeautifulSoup(Con,\"lxml\")\n",
    "Head_title = []\n",
    "PAR = []\n",
    "H = S.find_all(\"span\",{\"class\":\"mw-headline\"})\n",
    "P = S.find_all(\"p\")\n",
    "for i in range (len(H)):\n",
    "    Head_title.append(H[i].text)\n",
    "    PAR.append(P[i].text)\n",
    "F = {}\n",
    "for j in range(len(Head_title)):\n",
    "    F[Head_title[j]]=PAR[j]\n",
    "print(F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.4: Write a function to collect every link that redirects to another Wikipedia page\n",
    "\n",
    "from bs4 import  BeautifulSoup\n",
    "import requests\n",
    "LINK = requests.get(\"https://en.wikipedia.org/wiki/Python_(programming_language)\")\n",
    "Con = LINK.content\n",
    "S = BeautifulSoup(Con,\"lxml\")\n",
    "s = \"https://en.wikipedia.org\"\n",
    "L = []\n",
    "prefixes = '#'\n",
    "for link in S.find_all('a'):\n",
    "    L.append(link.get(\"href\"))\n",
    "L1 = []        \n",
    "for t in L:\n",
    "    if t != None and t[0] != \"#\":\n",
    "        L1.append(t)\n",
    "# for t in L1:\n",
    "#     print(t)\n",
    "L2 = []\n",
    "for i in L1:\n",
    "    if i[0] == '/':\n",
    "        n = s+i\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9de69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.5:  Wrap all the previous functions into a single function that takes as parameters a Wikipedia link\n",
    "\n",
    "def Web(lien):\n",
    "    # 1 function to Get and parse html content\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    LINK = requests.get(lien)\n",
    "    cont = LINK.content  #Web page content\n",
    "    S = BeautifulSoup(cont,'lxml')\n",
    "#     print(S)\n",
    "    print(\"2 function:\")\n",
    "    # 2: function to get the title\n",
    "    T = S.find('title')\n",
    "    print(T.get_text())\n",
    "    print(\"3 function:\")\n",
    "    # 3 ; function to get the heading and the paragraphs\n",
    "    Head_title = []\n",
    "    PAR = []\n",
    "    H = S.find_all(\"span\",{\"class\":\"mw-headline\"})\n",
    "    P = S.find_all(\"p\")\n",
    "    for i in range (len(H)):\n",
    "        Head_title.append(H[i].text)\n",
    "        PAR.append(P[i].text)\n",
    "    F = {}\n",
    "    for j in range(len(Head_title)):\n",
    "        F[Head_title[j]]=PAR[j]\n",
    "    print(F)\n",
    "     # 4: function to extract all links:\n",
    "    s = \"https://en.wikipedia.org\"\n",
    "    L = []\n",
    "    prefixes = '#'\n",
    "    for link in S.find_all('a'):\n",
    "        L.append(link.get(\"href\"))\n",
    "    L1 = []        \n",
    "    for t in L:\n",
    "        if t != None and t[0] != \"#\":\n",
    "            L1.append(t)\n",
    "    # for t in L1:\n",
    "    #     print(t)\n",
    "\n",
    "    L2 = []\n",
    "    for i in L1:\n",
    "        if i[0] == '/':\n",
    "            n = s+i\n",
    "            print(n)\n",
    "Web(\"https://en.wikipedia.org/wiki/Python_(programming_language)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
